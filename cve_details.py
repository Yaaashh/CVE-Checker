import aiohttp
import asyncio
import os
import time
from bs4 import BeautifulSoup, Tag
from typing import List, Tuple

BASIC_URL = "https://cve.mitre.org"
BASE_URL = BASIC_URL + "/cgi-bin/cvekey.cgi?keyword="
NVD_URL = "https://nvd.nist.gov/vuln/detail/"

REQUEST_TIMEOUT = aiohttp.ClientTimeout(total=30)
MAX_CVES_PER_TECHNOLOGY = 50

async def fetch_page(session: aiohttp.ClientSession, url: str) -> str:
    try:
        async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
            return await response.text()
    except Exception as e:
        print(f"Failed to fetch data from {url}: {e}")
        return ""

async def extract_cve_data(session: aiohttp.ClientSession, cve_id: str) -> Tuple[str, str, str]:
    url = NVD_URL + cve_id
    patch_link = "Patch Not Available"
    base_score = "N/A"
    published_date = "N/A"

    try:
        page_content = await fetch_page(session, url)
        if page_content:
            soup = BeautifulSoup(page_content, "html.parser")
            base_score_element = soup.find('a', {'data-testid': 'vuln-cvss3-cna-panel-score'}) or soup.find('a', {'data-testid': 'vuln-cvss3-panel-score'})
            base_score = base_score_element.get_text() if base_score_element else 'N/A'

            published_date_element = soup.find('span', {'data-testid': 'vuln-published-on'})
            published_date = published_date_element.get_text() if published_date_element else 'N/A'

            patch_link_element = soup.find('table', {'data-testid': 'vuln-hyperlinks-table'})
            if patch_link_element and isinstance(patch_link_element, Tag):
                links = patch_link_element.find_all('a')
                patch_link = links[0].get('href', 'Patch Not Available') if links else "Patch Not Available"
    except Exception as e:
        print(f"Error fetching CVE details for {cve_id}: {e}")

    return base_score, published_date, patch_link

async def process_keyword(session: aiohttp.ClientSession, keyword: str, url: str) -> Tuple[str, List[Tuple[str, str, str, str, str]]]:
    cve_data = []
    try:
        page_content = await fetch_page(session, url)
        if page_content:
            soup = BeautifulSoup(page_content, "html.parser")
            table = soup.find('div', {'id': 'TableWithRules'})
            if table:
                table = table.find('table')
                if table and isinstance(table, Tag):
                    rows = table.find_all('tr')[1:]

                    for i, row in enumerate(rows[:MAX_CVES_PER_TECHNOLOGY]):
                        if isinstance(row, Tag):  # Ensure row is a Tag object
                            columns = row.find_all('td')
                            if len(columns) >= 2:
                                cve_id = columns[0].text.strip()
                                cve_link = BASIC_URL + columns[0].find('a')['href']
                                base_score, published_date, patch_link = await extract_cve_data(session, cve_id)
                                cve_data.append((cve_id, cve_link, base_score, published_date, patch_link))

            os.makedirs('cves', exist_ok=True)
            file_path = f'cves/cve_data_{keyword}.txt'

            with open(file_path, 'w') as file:
                for cve_id, cve_link, base_score, published_date, patch_link in cve_data:
                    file.write(f"CVE ID: {cve_id}\n")
                    file.write(f"Link: {cve_link}\n")
                    file.write(f"Base Score: {base_score}\n")
                    file.write(f"Published Date: {published_date}\n")
                    file.write(f"Patch Link: {patch_link}\n\n")
            print(f'CVE details of {keyword} saved in {file_path}')
            return keyword, cve_data
        else:
            print(f"Failed to fetch data for keyword {keyword}.")
            return keyword, []
    except Exception as e:
        print(f"Error processing keyword {keyword}: {str(e)}")
        return keyword, []

async def gather_cves_for_keywords(keywords: List[str], urls: List[str]):
    failed_keywords = []
    start_time = time.time()

    async with aiohttp.ClientSession() as session:
        tasks = [process_keyword(session, keyword, url) for keyword, url in zip(keywords, urls)]
        results = await asyncio.gather(*tasks)

        total_keywords = len(keywords)
        successful_fetches = 0

        for keyword, cve_data in results:
            if cve_data:
                successful_fetches += 1
            else:
                failed_keywords.append(keyword)

        retry_time_limit = 180
        while failed_keywords and time.time() - start_time < retry_time_limit:
            retry_tasks = [process_keyword(session, keyword, BASE_URL + keyword) for keyword in failed_keywords]
            retry_results = await asyncio.gather(*retry_tasks)

            failed_keywords = []
            for keyword, cve_data in retry_results:
                if cve_data:
                    successful_fetches += 1
                else:
                    failed_keywords.append(keyword)

        # Display progress and any failures
        print(f"\nProgress: Fetched CVE data for {successful_fetches} out of {total_keywords} technologies.")
        if failed_keywords:
            print(f"\nFailed to fetch CVE details for the following technologies after retries:")
            for keyword in failed_keywords:
                print(keyword)
